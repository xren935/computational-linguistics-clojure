\documentclass[10pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{url}
% use straight quotes in texttt environment, make 0 different from O
\usepackage[zerostyle=b,straightquotes,scaled=.93]{newtxtt}
\usepackage{hyperref}

\usepackage{listings}
\lstset{
 basicstyle=\ttfamily,
 columns=fullflexible,
 upquote,
 keepspaces,
 literate={*}{{\char42}}1
 {-}{{\char45}}1
}
\usepackage[short labels]{enumitem}

\usepackage{soul}  % for strike through (\st{})
\usepackage[dvipsnames,usenames,table]{xcolor} % for colors


\usepackage{dsfont}

\newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}
\newcommand{\PSnum}{5}

\begin{document}

\title{LING/COMP 445, LING 645\\Problem Set \PSnum}
\date{Due before 8:35 AM on Tuesday, November 24, 2020}
\maketitle

This problem set consists only of questions involving mathematics or
English or or a combination of the two (no coding questions this time).
Please put your answers in an
\textbf{Answer} section like in the example below. You can find more
information about \LaTeX\ here \url{https://www.latex-project.org/}.

Once you have entered your answers, please compile your copy of this
\LaTeX{} into a PDF and submit 
\begin{enumerate}[(i),noitemsep]
\item
the compiled PDF renamed to
\texttt{ps\PSnum-lastname-firstname.pdf} and
\item
the raw \LaTeX{} file renamed to
\texttt{ps\PSnum-lastname-firstname.tex}
\end{enumerate}
to the Problem Set \PSnum{} folder under `Assignments' on MyCourses.


\noindent\hrulefill %--------------------------------

\paragraph{Example Problem:}
This is an example question using some fake math like this
$L=\sum_0^{\infty} \mathcal{G} \delta_x$.

\paragraph{Example Answer:} Put your answer right under the question like
this $L=\sum_0^{\infty} \mathcal{G} \delta_x$.

\noindent\hrulefill %--------------------------------

\paragraph{Problem 1:}
 
In class we gave the following equation for the bigram probability of
a sequence of words $W^{(1)},\dots,W^{(k)}$:

\begin{equation}\label{eq:bigram}
\Pr(W^{(1)},\dots,W^{(k)})=\prod_i^k \Pr(W^{(i)} | W^{(i-1)}=w^{(i-1)})
\end{equation}

\noindent Using this formula, give an expression for the bigram
probability of the sentence $abab$, where each character is treated as
a word. Try to simplify the formula as much as possible.

\paragraph{Answer 1:} Bigram model:$\Pr(W^{(i)} | W^{(1)}W^{(2)} \dots W^{(k)})$\approx$Pr(W^{(i)} | W^{(i-1)})$

$Pr(W^{(1)},\dots,W^{(k)})&=\prod_i^k \Pr(W^{(i)} | W^{(i-1)})$

$=Pr(a |{\rtimes})\cdot{\Pr(b | a)\cdot{\Pr(a | b)\cdot{\Pr(b | a)\cdot \Pr({\ltimes}|b)}}}$

$=Pr(a|{\rtimes})\cdot{(\Pr(b|a))^2\cdot{\Pr(a|b)\cdot{\Pr({\ltimes}|b)}}}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 2:}

Suppose that there are two possible symbols/words in our language, $a$
and $b$. There are three conditional distributions in the bigram model
for this language, $\Pr(W^{(i)} | W^{(i-1)}=a)$,
$\Pr(W^{(i)} | W^{(i-1)}=b)$, and $\Pr(W^{(i)} | W^{(i-1)}=\rtimes)$.
These conditional distributions are associated with the parameter
vectors $\vec{\theta}_{a}$, $\vec{\theta}_{b}$, and
$\vec{\theta}_{\rtimes}$, respectively (these parameter vectors were
implicit in the previous problem). For the current problem, we will
assume that these parameters are fixed.\\

\noindent Suppose that we are given a sentence $W^{(1)},\dots,W^{(k)}$. We will
use the notation $n_{x \rightarrow y}$ to denote the number of times
that the symbol $y$ occurs immediately following the symbol $x$ in the
sentence. For example, $n_{a \rightarrow a}$ counts the number of
times that symbol $a$ occurs immediately following the symbol $a$.
Using Equation \ref{eq:bigram}, give an expression for the probability
of a sentence in our language:

\begin{equation*}
\Pr(W^{(1)},\dots,W^{(k)} | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes})
\end{equation*}

\noindent The expression should make use of the $n_{x \rightarrow y}$ notation
defined above.\\

\noindent Hint: the expression should be analogous to the formula that we found
for the likelihood of a corpus under a bag of words model.

\paragraph{Answer 2:} $Pr(W^{(1)},\dots,W^{(k)} | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes}) &= Pr(S|\vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes})$

    $ =Pr(S|\vec{\theta_a})\cdot{\Pr(S|\vec{\theta_b})\cdot{\Pr(S|\vec{\theta}_{\rtimes})}}$
    
    $ = \prod_{w\in V} \theta_{a \rightarrow w}^{n_{a \rightarrow w}}\cdot{\prod_{w\in V} \theta_{b \rightarrow w}^{n_{b \rightarrow w}}\cdot{\prod_{w\in V} \theta_{\rtimes \rightarrow w}^{n_{\rtimes \rightarrow w}}}}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 3:}


Assume the parameter vectors in our bigram model have the following values:
\begin{align*}
&\vec{\theta}_{a} = (0.7,0.3)\\
&\vec{\theta}_{b} = (0.2,0.8)\\
&\vec{\theta}_{\rtimes} = (0.5,0.5)
\end{align*}

\noindent The first vector indicates that if the current symbol $a$,
there is probability $0.7$ of transitioning to the symbol $a$, and
probability $0.3$ of transitioning to the symbol $b$. Using your
answer to the previous problem and these parameter values, calculate
the probability of the string $aabab$.

\paragraph{Answer 3:} Using the previous result: 
$Pr(W^{(1)},\dots,W^{(k)} | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes}) 

&=Pr(S|\vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes}) 
= \prod_{w\in V} \theta_{a \rightarrow w}^{n_{a \rightarrow
w}}\cdot{\prod_{w\in V} \theta_{b \rightarrow w}^{n_{b \rightarrow w}}\cdot{\prod_{w\in V} \theta_{\rtimes \rightarrow w}^{n_{\rtimes \rightarrow w}}}}$


$Pr({\ltimes}aabab{\rtimes}}) = 0.5\cdot{0.7\cdot{(0.3)^{2}\cdot{0.2}}} = 0.0063$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 4:}

In Problem 2, you found an expression for the bigram probability of a
sentence in our language, which contains the symbols $a$ and $b$. In
that problem, we assumed that there were fixed parameter vectors
$\vec{\theta}$ associated with each conditional distribution. In this
problem, we will consider the setting in which we have uncertainty
about the value of these parameters.\\

\noindent In class, we used the Dirichlet distribution to define a
prior distribution over parameter vectors:

\begin{align}
\vec{\theta}_{\mathbf{c}} \mid \vec{\alpha} &\sim& \mathrm{Dirichlet}(\vec{\alpha}) \\
w^{(i)} \mid  w^{(i-1)} &\sim&\mathrm{categorical}(\vec{\theta}_{w^{(i-1)}}) & \\
w^{(1)} &\sim& \mathrm{categorical}(\vec{\theta}_{\rtimes})\ & 
\end{align}

\noindent Suppose that we have a sentence
$S=W^{(1)},\dots,W^{(k)}$. Given an expression for the joint
probability
$\Pr(S, \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes} |
\vec{\alpha})$
using the definitions of Dirichlet distributions and likelihoods we
defined in class.

\paragraph{Answer 4:} 
As seen in class, the Dirichlet distribution can be defined as:

$Pr({\theta}_{1},\dots,{\theta}_{K};{\alpha}_{1},\dots,{\alpha}_{K}) = \frac{1}{B_{\vec \alpha}}\prod_{i=1}^K \theta_{i}^{a_{i}-1}$

which is $= \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod _{i=1}^{K}\theta_{i}^{\alpha _{i}-1}$

$Pr(S, \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes} | \vec{\alpha})$

    $=Pr(S | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes})\cdot{\Pr(\vec{\theta}_{a}}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes}|\vec{\alpha})$
    
    $=Pr(S | \vec{\theta}_{a})\cdot{\Pr(\vec{\theta}_{a}}|\vec{\alpha})\cdot{Pr(S | \vec{\theta}_{b})\cdot{\Pr(\vec{\theta}_{b}}|\vec{\alpha})\cdot{Pr(S | \vec{\theta}_{\rtimes})\cdot{\Pr(\vec{\theta}_{\rtimes}}|\vec{\alpha})}}$
    
    $=\left(\frac{\Gamma(\sum_{w \in V} \alpha_{w})}{\prod_{w \in V} \Gamma(\alpha_{w})}\right)^3 \left(\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}\right)$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 5:}

In the previous problem, you found a formula for the joint probability
of a sentence and a set of bigram model parameters. Using this, give a
formula for the marginal probability of the sentence
$\Pr(S|\vec{\alpha})$.\\

\noindent Hint: The formula should be analogous to the formula derived
in class for marginal probability of a corpus under a bag of words
model. Whereas before there was only a single parameter vector
$\vec{\theta}$, now there are three parameters that need to be
marginalized away. Otherwise the calculation will be similar.

\paragraph{Answer 5:} 
$\Pr(S|\vec{\alpha})$
$=\int_{\theta} \Pr(S | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes})\cdot{\Pr(\vec{\theta}_{a}}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes}|\vec{\alpha})$

$=\int_{\theta_{a}} \Pr(S | \vec{\theta}_{a})\cdot{\Pr(\vec{\theta}_{a}}|\vec{\alpha})\cdot{\int_{\theta_{b}} \Pr(S | \vec{\theta}_{b})\cdot{\Pr(\vec{\theta}_{b}}|\vec{\alpha})\cdot{\int_{\theta_{\rtimes}} \Pr(S | \vec{\theta}_{\rtimes})\cdot{\Pr(\vec{\theta}_{\rtimes}}|\vec{\alpha})}}$

Using the previous result, $\int_{\Theta_a}\int_{\Theta_a}\int_{\Theta_a}\left(\frac{\Gamma(\sum_{w \in V} \alpha_{w})}{\prod_{w \in V} \Gamma(\alpha_{w})}\right)^3 \left(\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}\right)$

$=\left(\frac{\Gamma(\sum_{w \in V} \alpha_{w})}{\prod_{w \in V} \Gamma(\alpha_{w})}\right)^3\cdot{\left(\int_{\theta_{a}}\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\int_{\theta_{b}}\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\int_{\theta_{\rtimes}}\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}\right)}$

$= {\frac {\Gamma\left(\sum_{w \in V}\alpha _{w}\right)}{\prod _{w \in V} \Gamma (\alpha _{w})}}^3{\frac{\prod _{w \in S_\rtimes} \Gamma([n_{\rtimes \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_\rtimes}[n_{\rtimes \rightarrow w}+\alpha_{w}]\right)}}{\frac{\prod _{w \in S_b} \Gamma([n_{b \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_b}[n_{b \rightarrow w}+\alpha_{w}]\right)}}{\frac{\prod _{w \in S_a} \Gamma([n_{a \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_a}[n_{a \rightarrow w}+\alpha_{w}]\right)}}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 6:}

Let us assume that the parameters of the Dirichlet distribution are
$\vec{\alpha} = (1,1)$. Using your solution to the previous problem,
compute the marginal probability of the string $aabab$. The formula
that you compute should contain the
\href{https://en.wikipedia.org/wiki/Gamma_function}{gamma function}
$\Gamma(\cdot)$. Using the properties of the gamma function discussed
in class (i.e., it's relationship to the factorial) or an online
calculator, compute a numerical value for this expression.


\paragraph{Answer 6:} $Pr(aabab|(1,1)) = {\frac {\Gamma\left(\sum_{w \in V}\alpha _{w}\right)}{\prod _{w \in V} \Gamma (\alpha _{w})}}^3{\frac{\prod _{w \in S_\rtimes} \Gamma([n_{\rtimes \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_\rtimes}[n_{\rtimes \rightarrow w}+\alpha_{w}]\right)}}{\frac{\prod _{w \in S_b} \Gamma([n_{b \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_b}[n_{b \rightarrow w}+\alpha_{w}]\right)}}{\frac{\prod _{w \in S_a} \Gamma([n_{a \rightarrow w}+\alpha_{w}])}{\Gamma \left(\sum_{w\in S_a}[n_{a \rightarrow w}+\alpha_{w}]\right)}}$

$ ={\frac {\Gamma{2}}{\Gamma{1}\Gamma{1}}}^3 = 1$

$ Pr(aabab|(1,1))

=1^3\cdot{\frac{\Gamma{(1+1)}\cdot\Gamma{(2+1)}}{\Gamma{(1+1+2+1)}}}\cdot{\frac{\Gamma{(1+0)}\cdot\Gamma{(1+1)}}{\Gamma{(1+0+1+1)}}}\cdot{\frac{\Gamma{(1+0)}\cdot\Gamma{(1+1)}}{\Gamma{(1+0+1+1)}}}$

$ =\frac{1\cdot2}{24}\cdot\frac{1\cdot1}{2}\cdot\frac{1\cdot1}{2}$

$ =\frac{1}{48}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 7:}

Suppose that we have observed a sentence
$S=W^{(1)},\dots,W^{(k)}$. Find an expression for the posterior
distribution over the model parameters,
$\Pr(\vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes} \mid S,
\vec{\alpha})$.\\

\noindent Hint: Use the joint probability that you computed in Problem
4 and Bayes' rule. The solution should be analogous to the posterior
probability for the bag of words model.

\paragraph{Answer 7:} By Bayes' Rule: 

$Pr(\vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes} \mid S,\vec{\alpha}) = \frac{Pr(\vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes} \mid \vec{\alpha})}{Pr(S)}$

$=\frac{Pr(S \mid \vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes})\cdot Pr(\vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes} \mid \vec{\alpha})}{{\int_{\theta} \Pr(S | \vec{\theta}_{a}, \vec{\theta}_{b}, \vec{\theta}_{\rtimes})\cdot{\Pr(\vec{\theta}_{a}}\vec{\theta}_{b}, \vec{\theta}_{\rtimes}|\vec{\alpha})}}$

$=\frac{\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}}{\frac{\prod_{w \in V} \Gamma(n_A+\alpha_{A})}{\Gamma(\sum_{w \in V} (n_A+\alpha_{A}))}\cdot{\frac{\prod_{w \in V} \Gamma(n_B+\alpha_{B})}{\Gamma(\sum_{w \in V} (n_B+\alpha_{B}))}\cdot{\frac{\prod_{w \in V} \Gamma(n_S+\alpha_{S})}{\Gamma(\sum_{w \in V} (n_S+\alpha_{S}))}}}}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 8:}

Using your formula for the posterior distribution and setting
$\vec{\alpha} = (1,1)$, given an expression for the posterior
distribution over parameters given the sentence $aabab$. There should
be an easy way to interpret the posterior distribution, and how it was
derived from the observed sentence. What is this interpretation?

\paragraph{Answer 8:} Using the posterior and Bayesian inference, we can average over all posterior values and thus get a distribution over the multiple models instead of estimating a specific model.

$Pr(S|\vec{\alpha})=Pr(aabab|(1,1))=\frac{1}{48}$

From Question7, $Pr(\vec{\theta_a}, \vec{\theta_b}, \vec{\theta}_{\rtimes} \mid S,\vec{\alpha})=\frac{\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}}{\frac{\prod_{w \in V} \Gamma(n_A+\alpha_{A})}{\Gamma(\sum_{w \in V} (n_A+\alpha_{A}))}\cdot{\frac{\prod_{w \in V} \Gamma(n_B+\alpha_{B})}{\Gamma(\sum_{w \in V} (n_B+\alpha_{B}))}\cdot{\frac{\prod_{w \in V} \Gamma(n_S+\alpha_{S})}{\Gamma(\sum_{w \in V} (n_S+\alpha_{S}))}}}}$

Substituting $\frac{1}{48}$ as the denominator, we get $48 \cdot{\left(\prod_{w \in V} \theta_{A}^{n_A+\alpha_{A}-1}\cdot{\prod_{w \in V} \theta_{B}^{n_B+\alpha_{B}-1}\cdot{\prod_{w \in V} \theta_{S}^{n_S+\alpha_{S}-1}}}\right)}$ which is $48\cdot{(\theta_{\rtimes \rightarrow a}\cdot{\theta_{a \rightarrow a}\cdot{\theta_{a \rightarrow b}^2\cdot{\theta_{b \rightarrow a}}}})}$


\noindent\hrulefill %--------------------------------

\paragraph{Problem 9:}

Consider the language $L=\{a^* b a^*\}$, that is, the language
consisting of some number of a's, followed by a single b, followed by
some number of a's. Show that this language is not strictly
$2$-local.\\

\noindent Hint: use $n$-Local Suffix Substitution Closure ($n$-SSC).

\paragraph{Answer 9:} K-SSC says that if ${u_{1}xv_{1}}$ and ${u_{2}xv_{2}}$ were in the language for a substring x with length k-1 then the string ${u_{1}xv_{2}}$ was also in the language. That is if L is 2-local and a pivot ${x}$ is defined by a string of ${n-1}$ length, then the sentence ${u_{1}xv_{2}} \in L$.

Let $s1 = \rtimes aabaaa$ where $u_1 = \rtimes aab$, $x = a$ and $v_1 = aa$

and $s2 = \rtimes aaabaa$ where $u_2 = \rtimes aa$, $x = a$ and $v_2 = baa$

However $u_1 x v_2 = \rtimes aababaa \notin L$. Thus, L is not strictly 2-local.


\noindent\hrulefill %--------------------------------

\paragraph{Problem 10:}

Consider the language
$L= \{a^n b^m c^n d^m\ | n, m \in \mathbb{N} \}$, that is, the
language consisting of $n$ a's followed by $m$ b's followed by $n$ c's
followed by $m$ d's where $n$ and $m$ are
natural numbers. Show that this language is not strictly $2$-local.\\

\noindent Hint: use the same property as in the problem above.

\paragraph{Answer 10:} Using the same definition used in the previous problem,

Let $s1 = \rtimes abcd$, where $u_1 = \rtimes ab$, $x = c$ and $v_1 = d$

and $s2 = \rtimes aabccd$, where $u_2 = \rtimes aab$, $x = c$ and $v_2 = cd$

Take ${u_{1}xv_{2}}=\ltimes abccd$, in which a shows up once and c shows up twice, but L is defined as $L= \{a^n b^m c^n d^m\ | n, m \in \mathbb{N} \}$ where a and c show up an equal number of times. Thus, ${u_{1}xv_{2}} \notin L$. L is not strictly 2-local.


\noindent\hrulefill %--------------------------------

\paragraph{Problem 11:}

Show that the language $L= \{a^n b^m c^n d^m\ | n, m \in \mathbb{N} \}$ is not
strictly $k$-local, for any value of $k$.

\paragraph{Answer 11:} 
Let $s1 = \rtimes a^{k-1} b^{k-1} c^{k-1} d^{k-1} \ltimes$ 
and $s2 = \rtimes a^{k-2} b^{k-1} c^{k-2} d^{k-1} \ltimes$

Using n-SSC, as stated in the previous two questions, ${u_{1}xv_{2}}=\rtimes a^{k-1} b^{k-1} c^{k-2} d^{k-1} \ltimes$, and, clearly, $k-1$ does not equal $k-2$ for any value of k. Thus, ${u_{1}xv_{2}} \notin L$. L us not strictly k-local.


\noindent\hrulefill %--------------------------------

\paragraph{Problem 12:}

In class we proved that
$k\mathrm{-SSC}(L) \implies L \in \mathrm{SL}_k$. In other words, if a
language satisfies $k$-Local Suffix Substitution Closure, then it is
$k$-strictly local.\\

\noindent Use this theorem to prove that $k$-strictly local languages
are closed under intersection. More precisely, prove that if
$L_1 \in \mathrm{SL}_k$ and $L_2 \in \mathrm{SL}_k$, then
$L_1 \cap L_2 \in \mathrm{SL}_k$.

\paragraph{Answer 12:} K-SSC says that "if ${u_{1}xv_{1}}$ and ${u_{2}xv_{2}}$ were in the language for a substring x with length K-1 then the string ${u_{1}xv_{2}}$ was also in the language". Thus, by K-SSC, if $u_{1}xv_{1}$ is in $L_{k} and $u_{2}xv_{2}$ is in $L_{k}$, then $u_{1}xv_{2}$ is in $L_{k}, and thus $L \in SL_{k}$


Take $u_{1}xv_{1} \in L_{1}$ and $u_{2}xv_{2} \in L_{1}$ and $u_{1}xv_{2} \in L_{1}$.
$u_{1}'x'v_{1}' \in L_{2}$ and $u_{2}'x'v_{2}' \in L_{2}$ and $u_{1}'x'v_{2}' \in L_{2}$
The intersection of L1 and L2 would be what is both in L1 and L2, in this case, a subset of $u_{1}, u_{1}', u_{2}, u_{2}', x, x', v_{1}, v_{1}', v_{2}, v_{2}'$
$u_{1} x v_{1} \in L_1 \wedge u_{2} x v_{2} \in L1 \implies u_{1} x v_{2} \in L1 \cap$
and the same goes for L2, just with $u_{1}', u_{2}', x', v_{1}', v_{2}'$.
In the general form $u_{1} x v_{1} \in L1 \cap L2 \wedge u_{2} x v_{2} \in L1 \cap L2 \implies u_{1}x v_{2} \in L1 \cap L2$.
the intersection of L1 and L2 is made up of the sets that are in L1 and L2, and we know that L1 and L2 are both in $SL_{k}$.
Therefore if $L_1 \in \mathrm{SL}_k$ and $L_2 \in \mathrm{SL}_k$, then $L_1 \cap L_2 \in \mathrm{SL}_k$.

\end{document}
